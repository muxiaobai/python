{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文\n",
    "#### 部分文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo 分词\n",
    "import jieba\n",
    "\n",
    "with open('in_the_name_of_people/text1.txt',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "#   document_decode = document.decode('gbk')\n",
    "    document_cut = jieba.cut(document)\n",
    "    result = ' '.join(document_cut)\n",
    "#     result = result.encode('utf-8')\n",
    "    with open('in_the_name_of_people/nlp_test1.txt', 'w',encoding=\"utf-8\") as f2:\n",
    "        f2.write(result)\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 设置 人名地名等确定的分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 　可以发现对于一些人名和地名，jieba处理的不好，不过我们可以帮jieba加入词汇如下：\n",
    "jieba.suggest_freq('沙瑞金', True)\n",
    "jieba.suggest_freq('田国富', True)\n",
    "jieba.suggest_freq('高育良', True)\n",
    "jieba.suggest_freq('侯亮平', True)\n",
    "jieba.suggest_freq('钟小艾', True)\n",
    "jieba.suggest_freq('陈岩石', True)\n",
    "jieba.suggest_freq('欧阳菁', True)\n",
    "jieba.suggest_freq('易学习', True)\n",
    "jieba.suggest_freq('王大路', True)\n",
    "jieba.suggest_freq('蔡成功', True)\n",
    "jieba.suggest_freq('孙连城', True)\n",
    "jieba.suggest_freq('季昌明', True)\n",
    "jieba.suggest_freq('丁义珍', True)\n",
    "jieba.suggest_freq('郑西坡', True)\n",
    "jieba.suggest_freq('赵东来', True)\n",
    "jieba.suggest_freq('高小琴', True)\n",
    "jieba.suggest_freq('赵瑞龙', True)\n",
    "jieba.suggest_freq('林华华', True)\n",
    "jieba.suggest_freq('陆亦可', True)\n",
    "jieba.suggest_freq('刘新建', True)\n",
    "jieba.suggest_freq('刘庆祝', True)\n",
    "\n",
    "with open('in_the_name_of_people/text2.txt',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "    document_cut = jieba.cut(document)\n",
    "    result = ' '.join(document_cut)\n",
    "#     result = result.encode('utf-8')\n",
    "    with open('in_the_name_of_people/nlp_test2.txt', 'w',encoding=\"utf-8\") as f2:\n",
    "        f2.write(result)\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.导入停用词表 类似a an the 的 、地、得等 不用的分词 stop_words.txt 网上下载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '?', '、', '。', '“', '”', '《', '》', '！', '，', '：', '；', '？', '人民', '末##末', '啊', '阿', '哎', '哎呀', '哎哟', '唉', '俺', '俺们', '按', '按照', '吧', '吧哒', '把', '罢了', '被', '本', '本着', '比', '比方', '比如', '鄙人', '彼', '彼此', '边', '别', '别的', '别说', '并', '并且', '不比', '不成', '不单', '不但', '不独', '不管', '不光', '不过', '不仅', '不拘', '不论', '不怕', '不然', '不如', '不特', '不惟', '不问', '不只', '朝', '朝着', '趁', '趁着', '乘', '冲', '除', '除此之外', '除非', '除了', '此', '此间', '此外', '从', '从而', '打', '待', '但', '但是', '当', '当着', '到', '得', '的', '的话', '等', '等等', '地', '第', '叮咚', '对', '对于', '多', '多少', '而', '而况', '而且', '而是', '而外', '而言', '而已', '尔后', '反过来', '反过来说', '反之', '非但', '非徒', '否则', '嘎', '嘎登', '该', '赶', '个', '各', '各个', '各位', '各种', '各自', '给', '根据', '跟', '故', '故此', '固然', '关于', '管', '归', '果然', '果真', '过', '哈', '哈哈', '呵', '和', '何', '何处', '何况', '何时', '嘿', '哼', '哼唷', '呼哧', '乎', '哗', '还是', '还有', '换句话说', '换言之', '或', '或是', '或者', '极了', '及', '及其', '及至', '即', '即便', '即或', '即令', '即若', '即使', '几', '几时', '己', '既', '既然', '既是', '继而', '加之', '假如', '假若', '假使', '鉴于', '将', '较', '较之', '叫', '接着', '结果', '借', '紧接着', '进而', '尽', '尽管', '经', '经过', '就', '就是', '就是说', '据', '具体地说', '具体说来', '开始', '开外', '靠', '咳', '可', '可见', '可是', '可以', '况且', '啦', '来', '来着', '离', '例如', '哩', '连', '连同', '两者', '了', '临', '另', '另外', '另一方面', '论', '嘛', '吗', '慢说', '漫说', '冒', '么', '每', '每当', '们', '莫若', '某', '某个', '某些', '拿', '哪', '哪边', '哪儿', '哪个', '哪里', '哪年', '哪怕', '哪天', '哪些', '哪样', '那', '那边', '那儿', '那个', '那会儿', '那里', '那么', '那么些', '那么样', '那时', '那些', '那样', '乃', '乃至', '呢', '能', '你', '你们', '您', '宁', '宁可', '宁肯', '宁愿', '哦', '呕', '啪达', '旁人', '呸', '凭', '凭借', '其', '其次', '其二', '其他', '其它', '其一', '其余', '其中', '起', '起见', '岂但', '恰恰相反', '前后', '前者', '且', '然而', '然后', '然则', '让', '人家', '任', '任何', '任凭', '如', '如此', '如果', '如何', '如其', '如若', '如上所述', '若', '若非', '若是', '啥', '上下', '尚且', '设若', '设使', '甚而', '甚么', '甚至', '省得', '时候', '什么', '什么样', '使得', '是', '是的', '首先', '谁', '谁知', '顺', '顺着', '似的', '虽', '虽然', '虽说', '虽则', '随', '随着', '所', '所以', '他', '他们', '他人', '它', '它们', '她', '她们', '倘', '倘或', '倘然', '倘若', '倘使', '腾', '替', '通过', '同', '同时', '哇', '万一', '往', '望', '为', '为何', '为了', '为什么', '为着', '喂', '嗡嗡', '我', '我们', '呜', '呜呼', '乌乎', '无论', '无宁', '毋宁', '嘻', '吓', '相对而言', '像', '向', '向着', '嘘', '呀', '焉', '沿', '沿着', '要', '要不', '要不然', '要不是', '要么', '要是', '也', '也罢', '也好', '一', '一般', '一旦', '一方面', '一来', '一切', '一样', '一则', '依', '依照', '矣', '以', '以便', '以及', '以免', '以至', '以至于', '以致', '抑或', '因', '因此', '因而', '因为', '哟', '用', '由', '由此可见', '由于', '有', '有的', '有关', '有些', '又', '于', '于是', '于是乎', '与', '与此同时', '与否', '与其', '越是', '云云', '哉', '再说', '再者', '在', '在下', '咱', '咱们', '则', '怎', '怎么', '怎么办', '怎么样', '怎样', '咋', '照', '照着', '者', '这', '这边', '这儿', '这个', '这会儿', '这就是说', '这里', '这么', '这么点儿', '这么些', '这么样', '这时', '这些', '这样', '正如', '吱', '之', '之类', '之所以', '之一', '只是', '只限', '只要', '只有', '至', '至于', '诸位', '着', '着呢', '自', '自从', '自个儿', '自各儿', '自己', '自家', '自身', '综上所述', '总的来看', '总的来说', '总的说来', '总而言之', '总之', '纵', '纵令', '纵然', '纵使', '遵照', '作为', '兮', '呃', '呗', '咚', '咦', '喏', '啐', '喔唷', '嗬', '嗯', '嗳', '~', '!', '.', ':', '\"', \"'\", '(', ')', '*', 'A', '白', '社会主义', '--', '..', '>>', ' [', ' ]', '', '<', '>', '/', '\\\\', '|', '-', '_', '+', '=', '&', '^', '%', '#', '@', '`', ';', '$', '（', '）', '——', '—', '￥', '·', '...', '‘', '’', '〉', '〈', '…', '\\u3000', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '０', '１', '２', '３', '４', '５', '６', '７', '８', '９', '二', '三', '四', '五', '六', '七', '八', '九', '零', '＞', '＜', '＠', '＃', '＄', '％', '︿', '＆', '＊', '＋', '～', '｜', '［', '］', '｛', '｝', '啊哈', '啊呀', '啊哟', '挨次', '挨个', '挨家挨户', '挨门挨户', '挨门逐户', '挨着', '按理', '按期', '按时', '按说', '暗地里', '暗中', '暗自', '昂然', '八成', '白白', '半', '梆', '保管', '保险', '饱', '背地里', '背靠背', '倍感', '倍加', '本人', '本身', '甭', '比起', '比如说', '比照', '毕竟', '必', '必定', '必将', '必须', '便', '别人', '并非', '并肩', '并没', '并没有', '并排', '并无', '勃然', '不', '不必', '不常', '不大', '不但...而且', '不得', '不得不', '不得了', '不得已', '不迭', '不定', '不对', '不妨', '不管怎样', '不会', '不仅...而且', '不仅仅', '不仅仅是', '不经意', '不可开交', '不可抗拒', '不力', '不了', '不料', '不满', '不免', '不能不', '不起', '不巧', '不然的话', '不日', '不少', '不胜', '不时', '不是', '不同', '不能', '不要', '不外', '不外乎', '不下', '不限', '不消', '不已', '不亦乐乎', '不由得', '不再', '不择手段', '不怎么', '不曾', '不知不觉', '不止', '不止一次', '不至于', '才', '才能', '策略地', '差不多', '差一点', '常', '常常', '常言道', '常言说', '常言说得好', '长此下去', '长话短说', '长期以来', '长线', '敞开儿', '彻夜', '陈年', '趁便', '趁机', '趁热', '趁势', '趁早', '成年', '成年累月', '成心', '乘机', '乘胜', '乘势', '乘隙', '乘虚', '诚然', '迟早', '充分', '充其极', '充其量', '抽冷子', '臭', '初', '出', '出来', '出去', '除此', '除此而外', '除此以外', '除开', '除去', '除却', '除外', '处处', '川流不息', '传', '传说', '传闻', '串行', '纯', '纯粹', '此后', '此中', '次第', '匆匆', '从不', '从此', '从此以后', '从古到今', '从古至今', '从今以后', '从宽', '从来', '从轻', '从速', '从头', '从未', '从无到有', '从小', '从新', '从严', '从优', '从早到晚', '从中', '从重', '凑巧', '粗', '存心', '达旦', '打从', '打开天窗说亮话', '大', '大不了', '大大', '大抵', '大都', '大多', '大凡', '大概', '大家', '大举', '大略', '大面儿上', '大事', '大体', '大体上', '大约', '大张旗鼓', '大致', '呆呆地', '带', '殆', '待到', '单', '单纯', '单单', '但愿', '弹指之间', '当场', '当儿', '当即', '当口儿', '当然', '当庭', '当头', '当下', '当真', '当中', '倒不如', '倒不如说', '倒是', '到处', '到底', '到了儿', '到目前为止', '到头', '到头来', '得起', '得天独厚', '的确', '等到', '叮当', '顶多', '定', '动不动', '动辄', '陡然', '都', '独', '独自', '断然', '顿时', '多次', '多多', '多多少少', '多多益善', '多亏', '多年来', '多年前', '而后', '而论', '而又', '尔等', '二话不说', '二话没说', '反倒', '反倒是', '反而', '反手', '反之亦然', '反之则', '方', '方才', '方能', '放量', '非常', '非得', '分期', '分期分批', '分头', '奋勇', '愤然', '风雨无阻', '逢', '弗', '甫', '嘎嘎', '该当', '概', '赶快', '赶早不赶晚', '敢', '敢情', '敢于', '刚', '刚才', '刚好', '刚巧', '高低', '格外', '隔日', '隔夜', '个人', '各式', '更', '更加', '更进一步', '更为', '公然', '共', '共总', '够瞧的', '姑且', '古来', '故而', '故意', '固', '怪', '怪不得', '惯常', '光', '光是', '归根到底', '归根结底', '过于', '毫不', '毫无', '毫无保留地', '毫无例外', '好在', '何必', '何尝', '何妨', '何苦', '何乐而不为', '何须', '何止', '很', '很多', '很少', '轰然', '后来', '呼啦', '忽地', '忽然', '互', '互相', '哗啦', '话说', '还', '恍然', '会', '豁然', '活', '伙同', '或多或少', '或许', '基本', '基本上', '基于', '极', '极大', '极度', '极端', '极力', '极其', '极为', '急匆匆', '即将', '即刻', '即是说', '几度', '几番', '几乎', '几经', '既...又', '继之', '加上', '加以', '间或', '简而言之', '简言之', '简直', '见', '将才', '将近', '将要', '交口', '较比', '较为', '接连不断', '接下来', '皆可', '截然', '截至', '藉以', '借此', '借以', '届时', '仅', '仅仅', '谨', '进来', '进去', '近', '近几年来', '近来', '近年来', '尽管如此', '尽可能', '尽快', '尽量', '尽然', '尽如人意', '尽心竭力', '尽心尽力', '尽早', '精光', '经常', '竟', '竟然', '究竟', '就此', '就地', '就算', '居然', '局外', '举凡', '据称', '据此', '据实', '据说', '据我所知', '据悉', '具体来说', '决不', '决非', '绝', '绝不', '绝顶', '绝对', '绝非', '均', '喀', '看', '看来', '看起来', '看上去', '看样子', '可好', '可能', '恐怕', '快', '快要', '来不及', '来得及', '来讲', '来看', '拦腰', '牢牢', '老', '老大', '老老实实', '老是', '累次', '累年', '理当', '理该', '理应', '历', '立', '立地', '立刻', '立马', '立时', '联袂', '连连', '连日', '连日来', '连声', '连袂', '临到', '另方面', '另行', '另一个', '路经', '屡', '屡次', '屡次三番', '屡屡', '缕缕', '率尔', '率然', '略', '略加', '略微', '略为', '论说', '马上', '蛮', '满', '没', '没有', '每逢', '每每', '每时每刻', '猛然', '猛然间', '莫', '莫不', '莫非', '莫如', '默默地', '默然', '呐', '那末', '奈', '难道', '难得', '难怪', '难说', '内', '年复一年', '凝神', '偶而', '偶尔', '怕', '砰', '碰巧', '譬如', '偏偏', '乒', '平素', '颇', '迫于', '扑通', '其后', '其实', '奇', '齐', '起初', '起来', '起首', '起头', '起先', '岂', '岂非', '岂止', '迄', '恰逢', '恰好', '恰恰', '恰巧', '恰如', '恰似', '千', '千万', '千万千万', '切', '切不可', '切莫', '切切', '切勿', '窃', '亲口', '亲身', '亲手', '亲眼', '亲自', '顷', '顷刻', '顷刻间', '顷刻之间', '请勿', '穷年累月', '取道', '去', '权时', '全都', '全力', '全年', '全然', '全身心', '然', '人人', '仍', '仍旧', '仍然', '日复一日', '日见', '日渐', '日益', '日臻', '如常', '如此等等', '如次', '如今', '如期', '如前所述', '如上', '如下', '汝', '三番两次', '三番五次', '三天两头', '瑟瑟', '沙沙', '上', '上来', '上去']\n"
     ]
    }
   ],
   "source": [
    "#从文件导入停用词表\n",
    "stpwrdpath = \"stop_words/stop_words.txt\"\n",
    "stpwrd_dic = open(stpwrdpath)\n",
    "stpwrd_content = stpwrd_dic.read()\n",
    "#将停用词表转换为list  \n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()\n",
    "print(stpwrdlst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.计算每一段文档的ti-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。 \n",
      "\n",
      "沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州帝 豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。\n",
      "  (0, 37)\t0.16561160138799993\n",
      "  (0, 50)\t0.11638066362437578\n",
      "  (0, 32)\t0.33122320277599987\n",
      "  (0, 44)\t0.11638066362437578\n",
      "  (0, 54)\t0.11638066362437578\n",
      "  (0, 41)\t0.11638066362437578\n",
      "  (0, 33)\t0.11638066362437578\n",
      "  (0, 52)\t0.11638066362437578\n",
      "  (0, 9)\t0.11638066362437578\n",
      "  (0, 34)\t0.16561160138799993\n",
      "  (0, 46)\t0.11638066362437578\n",
      "  (0, 28)\t0.11638066362437578\n",
      "  (0, 21)\t0.11638066362437578\n",
      "  (0, 2)\t0.23276132724875156\n",
      "  (0, 14)\t0.11638066362437578\n",
      "  (0, 16)\t0.11638066362437578\n",
      "  (0, 0)\t0.23276132724875156\n",
      "  (0, 20)\t0.11638066362437578\n",
      "  (0, 47)\t0.11638066362437578\n",
      "  (0, 55)\t0.11638066362437578\n",
      "  (0, 53)\t0.11638066362437578\n",
      "  (0, 17)\t0.11638066362437578\n",
      "  (0, 18)\t0.11638066362437578\n",
      "  (0, 40)\t0.4140290034699999\n",
      "  (0, 4)\t0.23276132724875156\n",
      "  :\t:\n",
      "  (1, 37)\t0.10777808951126833\n",
      "  (1, 32)\t0.10777808951126833\n",
      "  (1, 34)\t0.10777808951126833\n",
      "  (1, 40)\t0.323334268533805\n",
      "  (1, 45)\t0.10777808951126833\n",
      "  (1, 36)\t0.45443503267993846\n",
      "  (1, 30)\t0.15147834422664616\n",
      "  (1, 10)\t0.15147834422664616\n",
      "  (1, 15)\t0.3029566884532923\n",
      "  (1, 8)\t0.15147834422664616\n",
      "  (1, 7)\t0.15147834422664616\n",
      "  (1, 35)\t0.3029566884532923\n",
      "  (1, 13)\t0.15147834422664616\n",
      "  (1, 43)\t0.15147834422664616\n",
      "  (1, 11)\t0.15147834422664616\n",
      "  (1, 48)\t0.15147834422664616\n",
      "  (1, 3)\t0.15147834422664616\n",
      "  (1, 29)\t0.3029566884532923\n",
      "  (1, 19)\t0.15147834422664616\n",
      "  (1, 24)\t0.15147834422664616\n",
      "  (1, 5)\t0.15147834422664616\n",
      "  (1, 23)\t0.15147834422664616\n",
      "  (1, 39)\t0.15147834422664616\n",
      "  (1, 25)\t0.15147834422664616\n",
      "  (1, 51)\t0.15147834422664616\n"
     ]
    }
   ],
   "source": [
    "with open('in_the_name_of_people/nlp_test1.txt',encoding=\"UTF-8\") as f3:\n",
    "    res1 = f3.read()\n",
    "print (res1)\n",
    "with open('in_the_name_of_people/nlp_test2.txt',encoding=\"UTF-8\") as f4:\n",
    "    res2 = f4.read()\n",
    "print(res2)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [res1,res2]\n",
    "# 加入停顿词\n",
    "vector = TfidfVectorizer(stop_words=stpwrdlst)\n",
    "tfidf = vector.fit_transform(corpus)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.输出ti-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------第 0 段文本的词语tf-idf权重------\n",
      "一起 0.23276132724875156\n",
      "万块 0.23276132724875156\n",
      "三人 0.23276132724875156\n",
      "三套 0.0\n",
      "下海经商 0.23276132724875156\n",
      "不想 0.0\n",
      "东挪西撮 0.11638066362437578\n",
      "之后 0.0\n",
      "事业有成 0.0\n",
      "事对 0.11638066362437578\n",
      "京州 0.0\n",
      "京州帝 0.0\n",
      "以沫 0.11638066362437578\n",
      "公司 0.0\n",
      "分开 0.11638066362437578\n",
      "别墅 0.0\n",
      "前一晚 0.11638066362437578\n",
      "县当 0.11638066362437578\n",
      "县长 0.11638066362437578\n",
      "名下 0.0\n",
      "喝酒 0.11638066362437578\n",
      "回忆起 0.11638066362437578\n",
      "困难 0.11638066362437578\n",
      "太大 0.0\n",
      "好像 0.0\n",
      "家住 0.0\n",
      "容易 0.11638066362437578\n",
      "对不起 0.23276132724875156\n",
      "很大 0.11638066362437578\n",
      "房子 0.0\n",
      "打听 0.0\n",
      "时期 0.11638066362437578\n",
      "易学习 0.33122320277599987\n",
      "有福 0.11638066362437578\n",
      "李达康 0.16561160138799993\n",
      "欧阳菁 0.0\n",
      "毛娅 0.0\n",
      "沙瑞金 0.16561160138799993\n",
      "没想到 0.11638066362437578\n",
      "浪费 0.0\n",
      "王大路 0.4140290034699999\n",
      "百姓 0.11638066362437578\n",
      "相助 0.11638066362437578\n",
      "股权 0.0\n",
      "胸怀 0.11638066362437578\n",
      "觉得 0.16561160138799993\n",
      "触动 0.11638066362437578\n",
      "话别 0.11638066362437578\n",
      "豪园 0.0\n",
      "赔礼道歉 0.11638066362437578\n",
      "赞叹 0.11638066362437578\n",
      "踏实 0.0\n",
      "这件 0.11638066362437578\n",
      "道口 0.11638066362437578\n",
      "金山 0.11638066362437578\n",
      "降职 0.11638066362437578\n",
      "风生水 0.11638066362437578\n",
      "-------第 1 段文本的词语tf-idf权重------\n",
      "一起 0.0\n",
      "万块 0.0\n",
      "三人 0.0\n",
      "三套 0.15147834422664616\n",
      "下海经商 0.0\n",
      "不想 0.15147834422664616\n",
      "东挪西撮 0.0\n",
      "之后 0.15147834422664616\n",
      "事业有成 0.15147834422664616\n",
      "事对 0.0\n",
      "京州 0.15147834422664616\n",
      "京州帝 0.15147834422664616\n",
      "以沫 0.0\n",
      "公司 0.15147834422664616\n",
      "分开 0.0\n",
      "别墅 0.3029566884532923\n",
      "前一晚 0.0\n",
      "县当 0.0\n",
      "县长 0.0\n",
      "名下 0.15147834422664616\n",
      "喝酒 0.0\n",
      "回忆起 0.0\n",
      "困难 0.0\n",
      "太大 0.15147834422664616\n",
      "好像 0.15147834422664616\n",
      "家住 0.15147834422664616\n",
      "容易 0.0\n",
      "对不起 0.0\n",
      "很大 0.0\n",
      "房子 0.3029566884532923\n",
      "打听 0.15147834422664616\n",
      "时期 0.0\n",
      "易学习 0.10777808951126833\n",
      "有福 0.0\n",
      "李达康 0.10777808951126833\n",
      "欧阳菁 0.3029566884532923\n",
      "毛娅 0.45443503267993846\n",
      "沙瑞金 0.10777808951126833\n",
      "没想到 0.0\n",
      "浪费 0.15147834422664616\n",
      "王大路 0.323334268533805\n",
      "百姓 0.0\n",
      "相助 0.0\n",
      "股权 0.15147834422664616\n",
      "胸怀 0.0\n",
      "觉得 0.10777808951126833\n",
      "触动 0.0\n",
      "话别 0.0\n",
      "豪园 0.15147834422664616\n",
      "赔礼道歉 0.0\n",
      "赞叹 0.0\n",
      "踏实 0.15147834422664616\n",
      "这件 0.0\n",
      "道口 0.0\n",
      "金山 0.0\n",
      "降职 0.0\n",
      "风生水 0.0\n"
     ]
    }
   ],
   "source": [
    "wordlist = vector.get_feature_names()#获取词袋模型中的所有词  \n",
    "# tf-idf矩阵 元素a[i][j]表示j词在i类文本中的tf-idf权重\n",
    "weightlist = tfidf.toarray()  \n",
    "#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重\n",
    "for i in range(len(weightlist)):  \n",
    "    print (\"-------第\",i,\"段文本的词语tf-idf权重------\")\n",
    "    for j in range(len(wordlist)):  \n",
    "        print (wordlist[j],weightlist[i][j] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。\n",
    "\n",
    "参考 https://www.jianshu.com/p/0d7b5c226f39\n",
    "\n",
    "\n",
    "TF（Term Frequency，词频）表示一个给定词语t在一篇给定文档d中出现的频率。TF越高，则词语t对文档d来说越重要，TF越低，则词语t对文档d来说越不重要。那是否可以以TF作为文本相似度评价标准呢？答案是不行的，举个例子，常用的中文词语如“我”，“了”，“是”等，在给定的一篇中文文档中出现的频率是很高的，但这些中文词几乎在每篇文档中都具有非常高的词频，如果以TF作为文本相似度评价标准，那么几乎每篇文档都能被命中。\n",
    "\n",
    "IDF（Inverse Document Frequency，逆向文件频率）的主要思想是：如果包含词语t的文档越少，则IDF越大，说明词语t在整个文档集层面上具有很好的类别区分能力。IDF说明了什么问题呢？还是举个例子，常用的中文词语如“我”，“了”，“是”等在每篇文档中几乎具有非常高的词频，那么对于整个文档集而言，这些词都是不重要的。对于整个文档集而言，评价词语重要性的标准就是IDF。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全部文档 http://files.cnblogs.com/files/pinard/in_the_name_of_people.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "#文件位置需要改为自己的存放路径\n",
    "#将文本分词\n",
    "with open('in_the_name_of_people\\in_the_name_of_people.txt',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "    document_cut = jieba.cut(document)\n",
    "    result = ' '.join(document_cut)\n",
    "    with open('in_the_name_of_people/in_the_name_of_people_segment.txt', 'w',encoding=\"utf-8\") as f2:\n",
    "        f2.write(result)\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 17:50:05,583 : INFO : collecting all words and their counts\n",
      "E:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-11-06 17:50:05,585 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-06 17:50:05,634 : INFO : collected 17878 word types from a corpus of 161343 raw words and 2311 sentences\n",
      "2019-11-06 17:50:05,635 : INFO : Loading a fresh vocabulary\n",
      "2019-11-06 17:50:05,665 : INFO : min_count=1 retains 17878 unique words (100% of original 17878, drops 0)\n",
      "2019-11-06 17:50:05,666 : INFO : min_count=1 leaves 161343 word corpus (100% of original 161343, drops 0)\n",
      "2019-11-06 17:50:05,715 : INFO : deleting the raw counts dictionary of 17878 items\n",
      "2019-11-06 17:50:05,716 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2019-11-06 17:50:05,717 : INFO : downsampling leaves estimated 120578 word corpus (74.7% of prior 161343)\n",
      "2019-11-06 17:50:05,735 : INFO : constructing a huffman tree from 17878 words\n",
      "2019-11-06 17:50:06,084 : INFO : built huffman tree with maximum node depth 17\n",
      "2019-11-06 17:50:06,119 : INFO : estimated required memory for 17878 words and 100 dimensions: 33968200 bytes\n",
      "2019-11-06 17:50:06,120 : INFO : resetting layer weights\n",
      "2019-11-06 17:50:06,274 : INFO : training model with 3 workers on 17878 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=10\n",
      "2019-11-06 17:50:06,484 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:50:06,487 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:50:06,507 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:50:06,508 : INFO : EPOCH - 1 : training on 161343 raw words (120448 effective words) took 0.2s, 517967 effective words/s\n",
      "2019-11-06 17:50:06,720 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:50:06,727 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:50:06,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:50:06,741 : INFO : EPOCH - 2 : training on 161343 raw words (120592 effective words) took 0.2s, 521236 effective words/s\n",
      "2019-11-06 17:50:06,944 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:50:06,948 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:50:06,970 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:50:06,971 : INFO : EPOCH - 3 : training on 161343 raw words (120565 effective words) took 0.2s, 530023 effective words/s\n",
      "2019-11-06 17:50:07,179 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:50:07,181 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:50:07,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:50:07,201 : INFO : EPOCH - 4 : training on 161343 raw words (120695 effective words) took 0.2s, 528394 effective words/s\n",
      "2019-11-06 17:50:07,397 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:50:07,401 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:50:07,418 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:50:07,419 : INFO : EPOCH - 5 : training on 161343 raw words (120551 effective words) took 0.2s, 558699 effective words/s\n",
      "2019-11-06 17:50:07,419 : INFO : training on a 806715 raw words (602851 effective words) took 1.1s, 526434 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import word2vec \n",
    "\n",
    "#加载语料\n",
    "sentences = word2vec.LineSentence('in_the_name_of_people/in_the_name_of_people_segment.txt')\n",
    "#训练语料\n",
    "path = get_tmpfile(\"word2vec.model\") #创建临时文件\n",
    "model = word2vec.Word2Vec(sentences, hs=1,min_count=1,window=10,size=100)\n",
    "# 保存模型 加载模型\n",
    "# model.save(\"word2vec.model\")\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 17:50:12,656 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('二十二日', 0.9415183663368225)\n",
      "('走过', 0.9195964336395264)\n",
      "('尸', 0.9174144268035889)\n",
      "('安插', 0.9173861742019653)\n",
      "('东湖', 0.9157454371452332)\n",
      "('一季', 0.9138946533203125)\n",
      "('予以', 0.9135046005249023)\n",
      "('歪着头', 0.9077277183532715)\n",
      "('他时', 0.9067841172218323)\n",
      "('死要', 0.9061307311058044)\n",
      "('身不由己', 0.902226448059082)\n",
      "('高额', 0.9020906686782837)\n",
      "('帝京', 0.9001924395561218)\n",
      "('局势', 0.8999568223953247)\n",
      "('小高潮', 0.8965737819671631)\n",
      "('站住脚', 0.8963971138000488)\n",
      "('空空荡荡', 0.8963592052459717)\n",
      "('码起', 0.8961787819862366)\n",
      "('万分', 0.8955192565917969)\n",
      "('屏住', 0.8931006193161011)\n",
      "('幽深', 0.8912655115127563)\n",
      "('雪片', 0.8907291889190674)\n",
      "('参照', 0.8883514404296875)\n",
      "('两种', 0.8874441385269165)\n",
      "('财富', 0.8855116963386536)\n",
      "('以至于', 0.8854597210884094)\n",
      "('渐行', 0.8851621747016907)\n",
      "('苑', 0.8836343288421631)\n",
      "('一仰', 0.8826392889022827)\n",
      "('一会儿站', 0.8823500871658325)\n",
      "('现已', 0.8815969228744507)\n",
      "('十月', 0.881014347076416)\n",
      "('厂出', 0.8798161745071411)\n",
      "('古今', 0.879292368888855)\n",
      "('栈桥', 0.8791755437850952)\n",
      "('地价', 0.8779714107513428)\n",
      "('挽回', 0.8776887655258179)\n",
      "('手握着', 0.8773161172866821)\n",
      "('床边', 0.8764464855194092)\n",
      "('号楼', 0.87632155418396)\n",
      "('签收', 0.8761618733406067)\n",
      "('倒风', 0.8758454322814941)\n",
      "('加入', 0.8757071495056152)\n",
      "('上衣', 0.8754230737686157)\n",
      "('调门', 0.8746518492698669)\n",
      "('吴慧芬系', 0.8744634389877319)\n",
      "('婉转', 0.8743815422058105)\n",
      "('草草', 0.8732860684394836)\n",
      "('水声', 0.8731328845024109)\n",
      "('驶临', 0.8729786276817322)\n",
      "('实属', 0.8728740215301514)\n",
      "('放眼', 0.8724775910377502)\n",
      "('敢于', 0.8724402189254761)\n",
      "('熄灭', 0.8720298409461975)\n",
      "('向前走', 0.8718390464782715)\n",
      "('市场', 0.8715548515319824)\n",
      "('飘落', 0.8714388608932495)\n",
      "('听见', 0.8708513975143433)\n",
      "('关照', 0.8705552816390991)\n",
      "('清高', 0.8699036836624146)\n",
      "('雄伟', 0.8693863153457642)\n",
      "('五十岁', 0.868869960308075)\n",
      "('爬起来', 0.8688338994979858)\n",
      "('细碎', 0.8682031631469727)\n",
      "('可据', 0.8678475618362427)\n",
      "('喷泉', 0.8668232560157776)\n",
      "('违规', 0.8666960000991821)\n",
      "('垮台', 0.8660874366760254)\n",
      "('内衣', 0.8659725785255432)\n",
      "('人死', 0.8658604621887207)\n",
      "('三十元', 0.8658267855644226)\n",
      "('窗口', 0.86581951379776)\n",
      "('插到', 0.8658060431480408)\n",
      "('还加', 0.8657844066619873)\n",
      "('这片', 0.8655093908309937)\n",
      "('护厂', 0.8651483654975891)\n",
      "('整间', 0.865001916885376)\n",
      "('一万六千', 0.8648799657821655)\n",
      "('九支', 0.8646355867385864)\n",
      "('辫子', 0.8645540475845337)\n",
      "('带离', 0.864415168762207)\n",
      "('银灰色', 0.8641784191131592)\n",
      "('三粒', 0.863760769367218)\n",
      "('浏览', 0.8635209798812866)\n",
      "('之争', 0.8635174036026001)\n",
      "('东边', 0.8620872497558594)\n",
      "('横亘', 0.8615298271179199)\n",
      "('领了', 0.8614586591720581)\n",
      "('魆', 0.8611339330673218)\n",
      "('联手', 0.8608862161636353)\n",
      "('收费站', 0.8607327938079834)\n",
      "('脑海', 0.8605476021766663)\n",
      "('日息', 0.8602190017700195)\n",
      "('穷', 0.8597921133041382)\n",
      "('星星', 0.8597393035888672)\n",
      "('成功者', 0.8597052693367004)\n",
      "('明争暗斗', 0.8592327833175659)\n",
      "('喜字', 0.8592242002487183)\n",
      "('鲁迅', 0.8591600656509399)\n",
      "('冷眼', 0.8589593768119812)\n"
     ]
    }
   ],
   "source": [
    "#输入与“贪污”相近的100个词\n",
    "for key in model.wv.similar_by_word('贪污', topn =100):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同上 只是多了打印日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 17:36:33,802 : INFO : collecting all words and their counts\n",
      "E:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-11-06 17:36:33,804 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-06 17:36:33,853 : INFO : collected 17878 word types from a corpus of 161343 raw words and 2311 sentences\n",
      "2019-11-06 17:36:33,853 : INFO : Loading a fresh vocabulary\n",
      "2019-11-06 17:36:33,885 : INFO : min_count=1 retains 17878 unique words (100% of original 17878, drops 0)\n",
      "2019-11-06 17:36:33,886 : INFO : min_count=1 leaves 161343 word corpus (100% of original 161343, drops 0)\n",
      "2019-11-06 17:36:33,931 : INFO : deleting the raw counts dictionary of 17878 items\n",
      "2019-11-06 17:36:33,932 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2019-11-06 17:36:33,932 : INFO : downsampling leaves estimated 120578 word corpus (74.7% of prior 161343)\n",
      "2019-11-06 17:36:33,949 : INFO : constructing a huffman tree from 17878 words\n",
      "2019-11-06 17:36:34,286 : INFO : built huffman tree with maximum node depth 17\n",
      "2019-11-06 17:36:34,318 : INFO : estimated required memory for 17878 words and 100 dimensions: 33968200 bytes\n",
      "2019-11-06 17:36:34,319 : INFO : resetting layer weights\n",
      "2019-11-06 17:36:34,471 : INFO : training model with 3 workers on 17878 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2019-11-06 17:36:34,651 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:36:34,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:36:34,669 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:36:34,670 : INFO : EPOCH - 1 : training on 161343 raw words (120329 effective words) took 0.2s, 611554 effective words/s\n",
      "2019-11-06 17:36:34,849 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:36:34,851 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:36:34,868 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:36:34,869 : INFO : EPOCH - 2 : training on 161343 raw words (120484 effective words) took 0.2s, 611138 effective words/s\n",
      "2019-11-06 17:36:35,048 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:36:35,049 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:36:35,066 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:36:35,068 : INFO : EPOCH - 3 : training on 161343 raw words (120571 effective words) took 0.2s, 613893 effective words/s\n",
      "2019-11-06 17:36:35,259 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:36:35,260 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:36:35,270 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:36:35,271 : INFO : EPOCH - 4 : training on 161343 raw words (120615 effective words) took 0.2s, 597851 effective words/s\n",
      "2019-11-06 17:36:35,507 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-06 17:36:35,508 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-06 17:36:35,524 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-06 17:36:35,525 : INFO : EPOCH - 5 : training on 161343 raw words (120765 effective words) took 0.3s, 478246 effective words/s\n",
      "2019-11-06 17:36:35,526 : INFO : training on a 806715 raw words (602764 effective words) took 1.1s, 571793 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging 训练模型\n",
    "import logging\n",
    "import os\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = word2vec.LineSentence('in_the_name_of_people/in_the_name_of_people_segment.txt') \n",
    "\n",
    "model = word2vec.Word2Vec(sentences, hs=1,min_count=1,window=3,size=100) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下为word2vec的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高育良 0.9703397750854492\n",
      "李达康 0.9573144912719727\n",
      "田国富 0.9502072930335999\n",
      "祁同伟 0.9378938674926758\n",
      "侯亮平 0.9365179538726807\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##找出某一个词向量最相近的词集合，代码如下 我们看看沙书记最相近的一些5个字的词（主要是人名）如下：\n",
    "req_count = 5\n",
    "for key in model.wv.similar_by_word('沙瑞金', topn =100):\n",
    "    if len(key[0])==3:\n",
    "        req_count -= 1\n",
    "        print (key[0], key[1])\n",
    "        if req_count == 0:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9703396804920335\n",
      "0.9380815330208045\n"
     ]
    }
   ],
   "source": [
    "#第二个应用是看两个词向量的相近程度，这里给出了书中两组人的相似程度：\n",
    "print(model.wv.similarity('沙瑞金', '高育良'))\n",
    "print(model.wv.similarity('李达康', '王大路'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刘庆祝\n"
     ]
    }
   ],
   "source": [
    "#第三个应用是找出不同类的词，这里给出了人物分类题：\n",
    "\n",
    "print(model.wv.doesnt_match(u\"沙瑞金 高育良 李达康 刘庆祝\".split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
